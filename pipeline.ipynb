{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc07b94f-32a5-4f83-ae6a-ce298f303a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports and getting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e1ae9d-a01f-4414-b656-e1305166c3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col, row_number, when, avg, sum\n",
    "from datetime import date\n",
    "\n",
    "# widgets with default value\n",
    "dbutils.widgets.text(\"date_param_min\", date.today().strftime(\"%Y-%m-%d\"), \"Minimum date\")\n",
    "dbutils.widgets.text(\"date_param_max\", date.today().strftime(\"%Y-%m-%d\"), \"Maximum date\")\n",
    "\n",
    "# read the widgets\n",
    "min_date = dbutils.widgets.get(\"date_param_min\")\n",
    "max_date = dbutils.widgets.get(\"date_param_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c464b8-5178-4038-9997-996ee7b44236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simulating Database to Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0224574e-42a6-495a-88ca-bc91e744bd55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get data from production database\n",
    "df = spark.table(\"samples.bakehouse.sales_transactions\")\n",
    "# add ingestion_timestamp and partition_date columns for audit and daily executions\n",
    "df_with_columns = df.withColumn(\"partition_date\", df[\"dateTime\"].cast(\"date\")).withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "# filter data based on date parameters\n",
    "filtered_df = df_with_columns.filter((df_with_columns[\"partition_date\"] >= min_date) & (df_with_columns[\"partition_date\"] <= max_date))\n",
    "# logging amount of data ingested\n",
    "print(\"Amount of data ingested: \", filtered_df.count())\n",
    "# write data to raw table\n",
    "filtered_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").partitionBy(\"partition_date\").saveAsTable(\"bakehouse.raw.sales_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af286b0-e6c3-40c6-9421-4ed990e8a95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simulating Raw to Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e80db6f-30e3-4583-8aa9-e76148c22a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get data from raw table\n",
    "df = spark.table(\"bakehouse.raw.sales_transactions\")\n",
    "# filter data based on date parameters\n",
    "filtered_df = df.filter((df[\"partition_date\"] >= min_date) & (df[\"partition_date\"] <= max_date))\n",
    "# rank each partition_date based on ingestion_timestamp\n",
    "ranked_df = filtered_df.withColumn(\"rank\", rank().over(Window.partitionBy(\"partition_date\").orderBy(col(\"ingestion_timestamp\").desc())))\n",
    "# deduplicate partition_date based on ingestion_timestamp\n",
    "files_deduped_df = ranked_df.filter(ranked_df[\"rank\"] == 1)\n",
    "# dropping rank column and updating ingestion_timestamp\n",
    "final_df = files_deduped_df.drop(\"rank\").withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "# logging amount of data ingested\n",
    "print(\"Amount of data ingested: \", final_df.count())\n",
    "# write data to bronze table\n",
    "final_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").partitionBy(\"partition_date\").option(\"partitionOverwriteMode\", \"dynamic\").saveAsTable(\"bakehouse.bronze.sales_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a86c24f2-66e5-46a6-aa5b-36a1099e7951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simulating Bronze to Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2594c70-1738-4d32-97d0-c25856f708c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get data from bronze table\n",
    "df = spark.table(\"bakehouse.bronze.sales_transactions\")\n",
    "# filter data based on date parameters\n",
    "filtered_df = df.filter((df[\"partition_date\"] >= min_date) & (df[\"partition_date\"] <= max_date)).filter(df[\"paymentMethod\"] != \"amex\")\n",
    "# rank each transactionID row based on ingestion_timestamp\n",
    "ranked_df = filtered_df.withColumn(\"rank\", row_number().over(Window.partitionBy(\"transactionID\").orderBy(col(\"ingestion_timestamp\").desc())))\n",
    "# deduplicate transactionID based on ingestion_timestamp\n",
    "deduped_df = ranked_df.filter(ranked_df[\"rank\"] == 1).drop(\"rank\")\n",
    "# check for percentage of negative values in quantity, totalPrice and unitPrice\n",
    "check_df = deduped_df.withColumn(\"is_valid\",when((deduped_df[\"quantity\"] < 0) | (deduped_df[\"totalPrice\"] < 0) | (deduped_df[\"unitPrice\"] < 0), 0).otherwise(1))\n",
    "valid_ratio = check_df.agg(avg(col(\"is_valid\")).alias(\"valid_ratio\")).collect()[0][\"valid_ratio\"]\n",
    "if valid_ratio < 0.8:\n",
    "  raise Exception(\"Percentage of valid records is less than 80%, check source\")\n",
    "else:\n",
    "  print(\"Percentage of valid records is greater than 80%, proceeding with the pipeline\")\n",
    "  # dropping is_valid column and updating ingestion_timestamp\n",
    "  final_df = check_df.drop(\"is_valid\").withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "  # logging amount of data ingested\n",
    "  print(\"Amount of data ingested: \", final_df.count())\n",
    "  # write data to silver table\n",
    "  final_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").partitionBy(\"partition_date\").option(\"partitionOverwriteMode\", \"dynamic\").saveAsTable(\"bakehouse.silver.sales_transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fabf066-1685-4ef1-acab-cff43e4ba966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simulating Silver to Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c384a3-ce04-40c4-a3e6-d61610e5d539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get data from silver table\n",
    "df = spark.table(\"bakehouse.silver.sales_transactions\")\n",
    "# filter data based on date parameters\n",
    "filtered_df = df.filter((df[\"partition_date\"] >= min_date) & (df[\"partition_date\"] <= max_date))\n",
    "# applying discount logic\n",
    "transformed_df = filtered_df.withColumn(\"discounted_total_price\", when(filtered_df[\"PaymentMethod\"] == \"visa\", filtered_df[\"totalPrice\"]*0.95).otherwise(filtered_df[\"totalPrice\"]))\n",
    "# aggregating data\n",
    "aggregated_df = transformed_df.groupBy(\"partition_date\",\"product\").agg(\n",
    "    sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    sum(\"totalPrice\").alias(\"total_revenue\"),\n",
    "    sum(\"discounted_total_price\").alias(\"total_discounted_revenue\")\n",
    ")\n",
    "# updating ingestion_timestamp\n",
    "final_df = aggregated_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "# logging amount of data ingested\n",
    "print(\"Amount of data ingested: \", final_df.count())\n",
    "# write data to gold table\n",
    "final_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").partitionBy(\"partition_date\").option(\"partitionOverwriteMode\", \"dynamic\").saveAsTable(\"bakehouse.gold.sales_transactions\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6374280136007406,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline",
   "widgets": {
    "date_param_max": {
     "currentValue": "2024-05-15",
     "nuid": "0e089367-87a1-426e-af06-f14a5c7b15c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-09-01",
      "label": "Maximum date",
      "name": "date_param_max",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-09-01",
      "label": "Maximum date",
      "name": "date_param_max",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "date_param_min": {
     "currentValue": "2024-05-14",
     "nuid": "65e398d2-5688-45c4-9281-9b50b320545b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-09-01",
      "label": "Minimum date",
      "name": "date_param_min",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-09-01",
      "label": "Minimum date",
      "name": "date_param_min",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
